{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>LITERATURE</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas Explored in this notebook : \n",
    "1. Extracted data from web API in JSON format\n",
    "2. Used python library Beautiful Soup for extracting data from the json file\n",
    "3. Pulled out all the latest posts and comments on the posts along with the timestamp. Converted original UNIX timestamp to a readable format.\n",
    "4. Cleaned the data for analysis - Removed punctuations, context specific stop words and unicode characters\n",
    "5. Performed tokenization and Lemmatization to find root words as per the context. Created the corpus\n",
    "6. Used TF_IDF vectorizer to convert each document in the corpus into a sparse matrix of TF-IDF features\n",
    "7. Performed SVD decomposition on the TF-IDF feature matrix\n",
    "8. Based on the eigen values and the eigen vectors, finding the words/phrases belongind to each topic,\n",
    "9. Derieving Inferences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "from datetime import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "### NLTK imports\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "### Gensim imports\n",
    "import gensim\n",
    "\n",
    "### SKLEARN imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>DATA EXTRACTION</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling data for Category : Food and Cooking\n",
    "# r = requests.get('https://a.4cdn.org/lit/catalog.json')\n",
    "# r = r.json()\n",
    "\n",
    "# pprint(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save into a JSON file\n",
    "# import json\n",
    "# import re\n",
    "# with open('Literature.json', 'w') as fout:\n",
    "#     json.dump(r , fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from a JSON file\n",
    "with open(r\"Literature.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More exploration\n",
    "# Pulling the comments and replies to the comments from the JSON file\n",
    "comments = []\n",
    "date_time = []\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i]['threads'])):\n",
    "        if 'com' in data[i]['threads'][j].keys() :\n",
    "            one_comm = data[i]['threads'][j]['com']\n",
    "            date_time += [datetime.datetime.fromtimestamp(data[i]['threads'][j]['time']).strftime(\"%B %d, %Y\")]\n",
    "            soup1 = BeautifulSoup(one_comm)\n",
    "            comments += [soup1.get_text()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "October 14, 2018\n",
      "/lit/ is for the discussion of literature, specifically books (fiction & non-fiction), short stories, poetry, creative writing, etc. If you want to discuss history, religion, or the humanities, go to /his/. If you want to discuss politics, go to /pol/. Philosophical discussion can go on either /lit/ or /his/, but those discussions of philosophy that take place on /lit/ should be based around specific philosophical works to which posters can refer.Check the wiki, the catalog, and the archive before asking for advice or recommendations, and please refrain from starting new threads for questions that can be answered by a search engine./lit/ is a slow board! Please take the time to read what others have written, and try to make thoughtful, well-written posts of your own. Bump replies are not necessary.Looking for books online? Check here:Guide to #bookzhttps://www.geocities.ws/prissy_90/Media/Texts/BookzHelp19kb.htmBookzzhttp://b-ok.org/Recommended Literaturehttp://4chanlit.wikia.com/wiki/Recommended_Reading\n"
     ]
    }
   ],
   "source": [
    "print(date_time[0])\n",
    "print(comments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'October 14, 2018'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first comment is from Oct 2018 and is the guidelines for using the forum. Removing it for the current data analysis.\n",
    "comments.pop(0)\n",
    "date_time.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "1. We have extracted 144 posts from the Literature section of the website.\n",
    "2. The comments were made on May 9th, 2020\n",
    "\n",
    "### Assumptions :\n",
    "We are working with the assumption that the comments below every post would be from the same topic as the post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>DATA PREPARATION/CLEANING</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.      CONVERTS WORDS TO THEIR BASE FORMS\n",
    "### 2. DATA CLEANING\n",
    "##### --> Removing punctuations, adding more context related stop words for removal, removing unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "with open(\"stop_words.txt\", \"r\") as input:\n",
    "        extra_stop_words = [line.split(\",\")[0] for line in input.read().splitlines()]\n",
    "stop_words = stopwords.words('english')\n",
    "newStopWords = ['/g/','n\\'t','\\'s','\\'\\'','``','would','get','like','use','one','\\'m','http','n','0', 'thus','x','1','say','good','much','want','go','run','need','new','even','shit','fuck']\n",
    "stop_words+=newStopWords\n",
    "stop_words+=extra_stop_words\n",
    "for text in comments:\n",
    "    cleaned_data += [clean_data(word_tokenize(text), stop_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>DATA EXPLORATION</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF VECTORIZING\n",
    "Using Scikit-learn's TF_IDF vectorizer to take my corpus and convert each document into a sparse matrix of TFIDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words,\n",
    "                                 use_idf=True, ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(comments)\n",
    "#print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7189"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = vectorizer.get_feature_names()\n",
    "len(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3413)\t0.3046444355243244\n",
      "  (0, 817)\t0.3046444355243244\n",
      "  (0, 6221)\t0.3046444355243244\n",
      "  (0, 2376)\t0.3046444355243244\n",
      "  (0, 3412)\t0.3046444355243244\n",
      "  (0, 816)\t0.3046444355243244\n",
      "  (0, 6220)\t0.3046444355243244\n",
      "  (0, 3673)\t0.2324118217657168\n",
      "  (0, 2373)\t0.25181235460358126\n",
      "  (0, 3407)\t0.26467850914370317\n",
      "  (0, 815)\t0.3046444355243244\n",
      "  (0, 6219)\t0.26467850914370317\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>SVD decomposition of TF-IDF feature vector</center></h1>\n",
    "\n",
    "Feature vector : X, a matrix where m is the number of documents and n is the number of terms\n",
    "\n",
    "Process : We will perform SVD decomposition if the matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 7189)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Vector\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why SVD decomposition for finding the topics?\n",
    "### The eigen vectors computed are orthogonal to each other and are in the direction of maximum variance and we need our final topics to be unrelated to each other and to cover the maximum range of discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=3, n_iter=100,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=3, n_iter=100)\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.49689484, 1.33859437, 1.31893058])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.singular_values_\n",
    "# The biggest singular value explains the maximum variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01224463, 0.0100076 , 0.01202409])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 7189)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the first row for V\n",
    "lsa.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.00045709 0.00045709 0.00045709 ... 0.00018687 0.00018687 0.00018687]\n",
      "1 [0.0017405  0.0017405  0.0017405  ... 0.00063329 0.00063329 0.00063329]\n",
      "2 [-0.00125716 -0.00125716 -0.00125716 ... -0.00018194 -0.00018194\n",
      " -0.00018194]\n"
     ]
    }
   ],
   "source": [
    "for i, comp in enumerate(lsa.components_): \n",
    "    print(i,comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['011',\n",
       " '011 lit',\n",
       " '011 lit ing',\n",
       " '10',\n",
       " '10 works',\n",
       " '10 works period',\n",
       " '12',\n",
       " '12 year',\n",
       " '12 year old',\n",
       " '15290943the',\n",
       " '15290943the next',\n",
       " '15290943the next settle',\n",
       " '15k',\n",
       " '1wvc5f4up2g8klfe',\n",
       " '1wvc5f4up2g8klfe 8s5wlylkp5oegtn4ctifggm2xs4',\n",
       " '1wvc5f4up2g8klfe 8s5wlylkp5oegtn4ctifggm2xs4 edit',\n",
       " '2014',\n",
       " '20th',\n",
       " '20th century',\n",
       " '20th century writers',\n",
       " '21st',\n",
       " '21st century',\n",
       " '21st century late',\n",
       " '31',\n",
       " '31 page',\n",
       " '31 page previews',\n",
       " '3m3l4vizv',\n",
       " '3m3l4vizv uhere',\n",
       " '3m3l4vizv uhere neat',\n",
       " '400',\n",
       " '400 absolute',\n",
       " '400 absolute essentials',\n",
       " '400 cigarettes',\n",
       " '400 cigarettes week',\n",
       " '45',\n",
       " '45 sunday',\n",
       " '45 sunday come',\n",
       " '80',\n",
       " '80 cigarettes',\n",
       " '80 cigarettes day',\n",
       " '8s5wlylkp5oegtn4ctifggm2xs4',\n",
       " '8s5wlylkp5oegtn4ctifggm2xs4 edit',\n",
       " '8s5wlylkp5oegtn4ctifggm2xs4 edit usp',\n",
       " '90',\n",
       " '90 95',\n",
       " '90 95 agents',\n",
       " '90 collected',\n",
       " '90 collected fictions',\n",
       " '95',\n",
       " '95 agents',\n",
       " '95 agents female',\n",
       " '9mm',\n",
       " '9mm automaticw',\n",
       " '9mm automaticw mean',\n",
       " 'a11t',\n",
       " 'a11t kmru',\n",
       " 'a11t kmru leat',\n",
       " 'aa',\n",
       " 'aa degree',\n",
       " 'aa degree spend',\n",
       " 'abandon',\n",
       " 'abandon approach',\n",
       " 'abandon approach substance',\n",
       " 'able',\n",
       " 'able develop',\n",
       " 'able develop serious',\n",
       " 'abollt',\n",
       " 'abollt number',\n",
       " 'abollt number times',\n",
       " 'abollt wbat',\n",
       " 'abollt wbat might',\n",
       " 'absolute',\n",
       " 'absolute essentials',\n",
       " 'absolute precision',\n",
       " 'absolute precision sonority',\n",
       " 'absolutely',\n",
       " 'absolutely overwhelmingly',\n",
       " 'absolutely overwhelmingly requested',\n",
       " 'absolutely read',\n",
       " 'absolutely read understand',\n",
       " 'absoluto',\n",
       " 'absoluto ni',\n",
       " 'absoluto ni una',\n",
       " 'absurdity',\n",
       " 'absurdity life',\n",
       " 'absurdity life drink',\n",
       " 'abu',\n",
       " 'abu huraira',\n",
       " 'abu huraira reported',\n",
       " 'abyss',\n",
       " 'abyss contribution',\n",
       " 'abyss contribution philosophy',\n",
       " 'abyss god',\n",
       " 'abyss god save',\n",
       " 'academia',\n",
       " 'academia got',\n",
       " 'academia got raped',\n",
       " 'academia writes',\n",
       " 'academia writes bunch',\n",
       " 'academic',\n",
       " 'academic books',\n",
       " 'academic books alchemy',\n",
       " 'acceleration',\n",
       " 'acceleration workget',\n",
       " 'acceleration workget catch',\n",
       " 'accept',\n",
       " 'accept absurdity',\n",
       " 'accept absurdity life',\n",
       " 'accepted',\n",
       " 'accepted form',\n",
       " 'accepted form manner',\n",
       " 'accepted nothing',\n",
       " 'accepted nothing must',\n",
       " 'accepted used',\n",
       " 'accepted used vastness',\n",
       " 'access',\n",
       " 'access earth',\n",
       " 'access earth core',\n",
       " 'accidentally',\n",
       " 'accidentally fritz',\n",
       " 'accidentally fritz continued',\n",
       " 'account',\n",
       " 'account fact',\n",
       " 'account fact airlines',\n",
       " 'account religion',\n",
       " 'account religion driven',\n",
       " 'account religion drove',\n",
       " 'acolytes',\n",
       " 'acolytes wittgenstein',\n",
       " 'acolytes wittgenstein think',\n",
       " 'across',\n",
       " 'across people',\n",
       " 'across people influential',\n",
       " 'act',\n",
       " 'act debate',\n",
       " 'act debate clowns',\n",
       " 'actuality',\n",
       " 'actuality organic',\n",
       " 'actuality organic philosophy',\n",
       " 'actualization',\n",
       " 'actualization gets',\n",
       " 'actualization gets sick',\n",
       " 'add',\n",
       " 'add lovecraft',\n",
       " 'add lovecraft beyond',\n",
       " 'added',\n",
       " 'added unhappiness',\n",
       " 'added unhappiness must',\n",
       " 'addicted',\n",
       " 'addicted la',\n",
       " 'addicted la hooker',\n",
       " 'admittance',\n",
       " 'admittance alone',\n",
       " 'admittance alone forgiven',\n",
       " 'advancing',\n",
       " 'advancing socialism',\n",
       " 'advancing socialism saw',\n",
       " 'afford',\n",
       " 'afford smoke',\n",
       " 'afford smoke 400',\n",
       " 'age',\n",
       " 'agents',\n",
       " 'agents female',\n",
       " 'agents female overwhelmingly',\n",
       " 'agents literary',\n",
       " 'agents literary establishment',\n",
       " 'agents push',\n",
       " 'agents push endless',\n",
       " 'agents talking',\n",
       " 'agents talking word',\n",
       " 'airlines',\n",
       " 'airlines failing',\n",
       " 'airlines failing order',\n",
       " 'alchemy',\n",
       " 'alchemy hardly',\n",
       " 'alchemy hardly begin',\n",
       " 'alchemy looking',\n",
       " 'alchemy looking encyclopedias',\n",
       " 'alcohol',\n",
       " 'alcohol hoy',\n",
       " 'alcohol hoy todo',\n",
       " 'alien',\n",
       " 'alien entities',\n",
       " 'alien entities stay',\n",
       " 'alien organic',\n",
       " 'alien organic philosophy',\n",
       " 'alive',\n",
       " 'alive learns',\n",
       " 'alive learns anything',\n",
       " 'allah',\n",
       " 'allah forbiddeth',\n",
       " 'allah forbiddeth warred',\n",
       " 'allah loveth',\n",
       " 'allah loveth dealers',\n",
       " 'allah peace',\n",
       " 'allah peace blessings',\n",
       " 'allah think',\n",
       " 'allah think man',\n",
       " 'allah universe',\n",
       " 'allah universe instead',\n",
       " 'allá',\n",
       " 'allá en',\n",
       " 'allá en lo',\n",
       " 'almost',\n",
       " 'almost fell',\n",
       " 'almost fell chair',\n",
       " 'alone',\n",
       " 'alone aunt',\n",
       " 'alone aunt rosalie',\n",
       " 'alone forgiven',\n",
       " 'alone forgiven sadly',\n",
       " 'alone pray',\n",
       " 'alone pray succour',\n",
       " 'alonso',\n",
       " 'alonso poem',\n",
       " 'alonso poem belongs',\n",
       " 'alphabets',\n",
       " 'alphabets map',\n",
       " 'alphabets map clock',\n",
       " 'alright',\n",
       " 'alright guys',\n",
       " 'alright guys post',\n",
       " 'als',\n",
       " 'als daf',\n",
       " 'als daf zu',\n",
       " 'also',\n",
       " 'also cursed',\n",
       " 'also cursed dyspraxia',\n",
       " 'also figure',\n",
       " 'also figure limited',\n",
       " 'also islam',\n",
       " 'also islam abu',\n",
       " 'also rare',\n",
       " 'also rare earth',\n",
       " 'also roll',\n",
       " 'also someone',\n",
       " 'also someone points',\n",
       " 'also sometimes',\n",
       " 'also sometimes finish',\n",
       " 'also statism',\n",
       " 'also statism also',\n",
       " 'also think',\n",
       " 'also think exactly',\n",
       " 'altering',\n",
       " 'altering brain',\n",
       " 'altering brain chemistry',\n",
       " 'alternative',\n",
       " 'alternative capitalism',\n",
       " 'alternative capitalism critique',\n",
       " 'alternative capitalism socialism',\n",
       " 'although',\n",
       " 'although truly',\n",
       " 'although truly love',\n",
       " 'always',\n",
       " 'always elected',\n",
       " 'always elected protagonist',\n",
       " 'amazon',\n",
       " 'amazon com',\n",
       " 'amazon com gp',\n",
       " 'americanism',\n",
       " 'americanism heidegger',\n",
       " 'americanism heidegger called',\n",
       " 'among',\n",
       " 'among many',\n",
       " 'among many arising',\n",
       " 'analogs',\n",
       " 'analogs iced',\n",
       " 'analogs iced three',\n",
       " 'analogy',\n",
       " 'analogy borrowed',\n",
       " 'analogy borrowed assume',\n",
       " 'analysis',\n",
       " 'analysis without',\n",
       " 'analysis without purchasing',\n",
       " 'analytics',\n",
       " 'analytics philosophy',\n",
       " 'analytics philosophy since',\n",
       " 'anarion',\n",
       " 'anarion failed',\n",
       " 'anarion failed tree',\n",
       " 'ancap',\n",
       " 'ancap views',\n",
       " 'ancap views books',\n",
       " 'ancestors',\n",
       " 'ancestors racemixed',\n",
       " 'anfangen',\n",
       " 'anfangen bitte',\n",
       " 'anfangen bitte nur',\n",
       " 'anime',\n",
       " 'anime authors',\n",
       " 'anime authors books',\n",
       " 'anime lasses',\n",
       " 'anime lasses grown',\n",
       " 'anime realism',\n",
       " 'anime realism post',\n",
       " 'anon',\n",
       " 'anon must',\n",
       " 'anon must beg',\n",
       " 'another',\n",
       " 'another contention',\n",
       " 'another contention used',\n",
       " 'another several',\n",
       " 'another several decades',\n",
       " 'another worldby',\n",
       " 'another worldby mushroom',\n",
       " 'answer',\n",
       " 'answer god',\n",
       " 'answer god omnipotent',\n",
       " 'answer institution',\n",
       " 'answer institution begs',\n",
       " 'anybody',\n",
       " 'anybody book',\n",
       " 'anybody book poem',\n",
       " 'anybody read',\n",
       " 'anybody read philosophical',\n",
       " 'anybody source',\n",
       " 'anybody source fact',\n",
       " 'anybody trying',\n",
       " 'anybody trying writing',\n",
       " 'anyone',\n",
       " 'anyone consider',\n",
       " 'anyone consider art',\n",
       " 'anyone ever',\n",
       " 'anyone ever thought',\n",
       " 'anyone file',\n",
       " 'anyone file look',\n",
       " 'anyone needs',\n",
       " 'anyone needs illustrations',\n",
       " 'anyone solution',\n",
       " 'anyone solution booting',\n",
       " 'anyone taken',\n",
       " 'anyone taken laschpill',\n",
       " 'anything',\n",
       " 'anything anime',\n",
       " 'anything anime realism',\n",
       " 'anything else',\n",
       " 'anything else absolutely',\n",
       " 'anything science',\n",
       " 'anything science cannot',\n",
       " 'anything similar',\n",
       " 'anything similar https',\n",
       " 'anywhere',\n",
       " 'anywhere cannot',\n",
       " 'anywhere cannot exist',\n",
       " 'ape',\n",
       " 'ape screech',\n",
       " 'ape screech known',\n",
       " 'appear',\n",
       " 'appear many',\n",
       " 'appear many many',\n",
       " 'appears',\n",
       " 'appears sitting',\n",
       " 'appears sitting wheelchair',\n",
       " 'approach',\n",
       " 'approach substance',\n",
       " 'approach substance quality',\n",
       " 'approved',\n",
       " 'approved cinema',\n",
       " 'arabic',\n",
       " 'arabic arabic',\n",
       " 'arabic arabic words',\n",
       " 'arabic linked',\n",
       " 'arabic linked sahih',\n",
       " 'arabic rhythm',\n",
       " 'arabic rhythm thee',\n",
       " 'arabic uses',\n",
       " 'arabic uses past',\n",
       " 'arabic words',\n",
       " 'arabic words uses',\n",
       " 'arabic writing',\n",
       " 'arabic writing extremely',\n",
       " 'aragorn',\n",
       " 'aragorn people',\n",
       " 'aragorn people routinely',\n",
       " 'arberry',\n",
       " 'arberry fatiha',\n",
       " 'arberry fatiha compare',\n",
       " 'arberry said',\n",
       " 'arberry said conveys',\n",
       " 'argentine',\n",
       " 'argentine streets',\n",
       " 'argentine streets lozenges',\n",
       " 'argue',\n",
       " 'argue ely',\n",
       " 'argue ely ong',\n",
       " 'argues',\n",
       " 'argues intellect',\n",
       " 'argues intellect holds',\n",
       " 'argument',\n",
       " 'argument humans',\n",
       " 'argument humans think',\n",
       " 'argument socialism',\n",
       " 'arguments',\n",
       " 'arguments fritz',\n",
       " 'arguments fritz hare',\n",
       " 'arguments getting',\n",
       " 'arguments getting aa',\n",
       " 'arising',\n",
       " 'arising composition',\n",
       " 'arising composition many',\n",
       " 'aristotle',\n",
       " 'aristotle anything',\n",
       " 'aristotle anything else',\n",
       " 'aristotle categories',\n",
       " 'aroused',\n",
       " 'aroused fire',\n",
       " 'aroused fire rouge',\n",
       " 'art',\n",
       " 'art another',\n",
       " 'art another several',\n",
       " 'art anyone',\n",
       " 'art anyone ever',\n",
       " 'art wrathful',\n",
       " 'art wrathful astray',\n",
       " 'articles',\n",
       " 'articles etc',\n",
       " 'articles etc begin',\n",
       " 'artist',\n",
       " 'artist writer',\n",
       " 'artist writer asking',\n",
       " 'aryans',\n",
       " 'aryans three',\n",
       " 'aryans three hindu',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asked mother',\n",
       " 'asked mother leave',\n",
       " 'asking',\n",
       " 'asking anyone',\n",
       " 'asking anyone needs',\n",
       " 'aspect',\n",
       " 'aspect either',\n",
       " 'aspect either tongue',\n",
       " 'ass',\n",
       " 'ass man',\n",
       " 'ass man wow',\n",
       " 'assume',\n",
       " 'assume substances',\n",
       " 'assume substances human',\n",
       " 'asteroid',\n",
       " 'asteroid access',\n",
       " 'asteroid access earth',\n",
       " 'asteroid mines',\n",
       " 'asteroid mines factories',\n",
       " 'asteroids',\n",
       " 'asteroids ever',\n",
       " 'asteroids ever mined',\n",
       " 'asteroids gold',\n",
       " 'asteroids gold asteroids',\n",
       " 'astray',\n",
       " 'astray existentialist',\n",
       " 'astray existentialist philosophy',\n",
       " 'astray line',\n",
       " 'astray line breaks',\n",
       " 'ate',\n",
       " 'ate booksthis',\n",
       " 'ate booksthis fucking',\n",
       " 'atheism',\n",
       " 'atheism inability',\n",
       " 'atheism inability believe',\n",
       " 'attemptand',\n",
       " 'attemptand yes',\n",
       " 'attemptand yes look',\n",
       " 'attitude',\n",
       " 'attitude blitzed',\n",
       " 'attitude blitzed polydrug',\n",
       " 'ature',\n",
       " 'augustine',\n",
       " 'augustine science',\n",
       " 'augustine science disciplina',\n",
       " 'aum',\n",
       " 'aum rosalie',\n",
       " 'aum rosalie said',\n",
       " 'aunt',\n",
       " 'aunt rosalie',\n",
       " 'aunt rosalie always',\n",
       " 'aunt rosalie called',\n",
       " 'aunt rosalie welcome',\n",
       " 'australia',\n",
       " 'australia philosophy',\n",
       " 'author',\n",
       " 'author basic',\n",
       " 'author basic argument',\n",
       " 'author iranian',\n",
       " 'author iranian shia',\n",
       " 'authors',\n",
       " 'authors books',\n",
       " 'authors intended',\n",
       " 'authors intended write',\n",
       " 'automaticw',\n",
       " 'automaticw mean',\n",
       " 'available',\n",
       " 'available 31',\n",
       " 'available 31 page',\n",
       " 'aw',\n",
       " 'aw b084zyv15n',\n",
       " 'aw b084zyv15n ref',\n",
       " 'awaketonight',\n",
       " 'awaketonight meeting',\n",
       " 'awaketonight meeting seventh',\n",
       " 'away',\n",
       " 'away becoming',\n",
       " 'away becoming reality',\n",
       " 'awkwardly',\n",
       " 'awkwardly moving',\n",
       " 'awkwardly moving past',\n",
       " 'ay',\n",
       " 'ay noches',\n",
       " 'ay noches de',\n",
       " 'ayah',\n",
       " 'ayah https',\n",
       " 'ayah https youtube',\n",
       " 'aztec',\n",
       " 'aztec calendars',\n",
       " 'aztec calendars tonalpohualli',\n",
       " 'aztec philosophy',\n",
       " 'aztec philosophy conceives',\n",
       " 'b084zyv15n',\n",
       " 'b084zyv15n ref',\n",
       " 'b084zyv15n ref dbs_a_w_dp_b084zyv15n',\n",
       " 'ba',\n",
       " 'ba real',\n",
       " 'ba real options',\n",
       " 'baby',\n",
       " 'baby pseudo',\n",
       " 'baby pseudo intellectual',\n",
       " 'bacon',\n",
       " 'bacon opus',\n",
       " 'bacon opus majusjacob',\n",
       " 'bad',\n",
       " 'bad attitude',\n",
       " 'bad attitude blitzed',\n",
       " 'bad boy',\n",
       " 'bad boy homie',\n",
       " 'bad meme',\n",
       " 'bad seems',\n",
       " 'bad seems cares',\n",
       " 'bang',\n",
       " 'bang widely',\n",
       " 'bang widely accepted',\n",
       " 'based',\n",
       " 'based bhagavad',\n",
       " 'based bhagavad gita',\n",
       " 'based cringe',\n",
       " 'based department',\n",
       " 'based etc',\n",
       " 'based etc question',\n",
       " 'based piques',\n",
       " 'based piques interest',\n",
       " 'basic',\n",
       " 'basic argument',\n",
       " 'basic argument humans',\n",
       " 'basic concepts',\n",
       " 'basic concepts heidegger',\n",
       " 'basically',\n",
       " 'basically bloomer',\n",
       " 'basically bloomer meme',\n",
       " 'basically equivalent',\n",
       " 'basically equivalent tariff',\n",
       " 'batter',\n",
       " 'batter face',\n",
       " 'batter face full',\n",
       " 'beauty',\n",
       " 'beauty erotic',\n",
       " 'beauty erotic force',\n",
       " 'became',\n",
       " 'became mingled',\n",
       " 'became mingled lesser',\n",
       " 'beckoned',\n",
       " 'beckoned justice',\n",
       " 'beckoned justice conversation',\n",
       " 'become',\n",
       " 'become better',\n",
       " 'become better informed',\n",
       " 'become known',\n",
       " 'become known member',\n",
       " 'becomes',\n",
       " 'becomes unveiled',\n",
       " 'becomes unveiled star',\n",
       " 'becoming',\n",
       " 'becoming divorced',\n",
       " 'becoming divorced reality',\n",
       " 'becoming reality',\n",
       " 'becoming time',\n",
       " 'becoming time place',\n",
       " 'beer',\n",
       " 'beer whats',\n",
       " 'beer whats name',\n",
       " 'beg',\n",
       " 'beg wisdom',\n",
       " 'beg wisdom troubling',\n",
       " 'begin',\n",
       " 'begin reading',\n",
       " 'begin reading original',\n",
       " 'begin thoughts',\n",
       " 'beginning',\n",
       " 'beginning customary',\n",
       " 'beginning customary part',\n",
       " 'beginning may',\n",
       " 'beginning may sound',\n",
       " 'begs',\n",
       " 'begs question',\n",
       " 'begs question cap',\n",
       " 'beings',\n",
       " 'beings general',\n",
       " 'beings general author',\n",
       " 'belated',\n",
       " 'belated rival',\n",
       " 'belated rival nietzsche',\n",
       " 'believable',\n",
       " 'believe',\n",
       " 'believe happening',\n",
       " 'believe happening live',\n",
       " 'believe pseudoscience',\n",
       " 'believe serious',\n",
       " 'believe serious symptom',\n",
       " 'believed',\n",
       " 'believed another',\n",
       " 'believed another contention',\n",
       " 'believes',\n",
       " 'believes fighting',\n",
       " 'believes fighting nihilism',\n",
       " 'belongs',\n",
       " 'belongs god',\n",
       " 'belongs god lord',\n",
       " 'belongs vicentico',\n",
       " 'belongs vicentico vicentico',\n",
       " 'bentham',\n",
       " 'bentham manual',\n",
       " 'bentham manual political',\n",
       " 'better',\n",
       " 'better artist',\n",
       " 'better artist writer',\n",
       " 'better books',\n",
       " 'better books listening',\n",
       " 'better informed',\n",
       " 'better informed religion',\n",
       " 'better job',\n",
       " 'better job fucking',\n",
       " 'better teachers',\n",
       " 'better understand',\n",
       " 'better understand motherfucker',\n",
       " 'beyond',\n",
       " 'beyond godly',\n",
       " 'beyond godly alien',\n",
       " 'bhagavad',\n",
       " 'bhagavad gita',\n",
       " 'bhagavad gita based',\n",
       " 'bhagavad gita prabhupada',\n",
       " 'big',\n",
       " 'big bang',\n",
       " 'big bang widely',\n",
       " 'big beer',\n",
       " 'big beer whats',\n",
       " 'big share',\n",
       " 'big share money',\n",
       " 'big titty',\n",
       " 'big titty anime',\n",
       " 'billigst',\n",
       " 'billigst und',\n",
       " 'billigst und leichtest',\n",
       " 'binds',\n",
       " 'binds wall',\n",
       " 'binds wall chains',\n",
       " 'bitte',\n",
       " 'bitte nur',\n",
       " 'bitte nur bücher',\n",
       " 'bl1ny77mkrw',\n",
       " 'bl1ny77mkrw feature',\n",
       " 'bl1ny77mkrw feature emb_title',\n",
       " 'black',\n",
       " 'black sun',\n",
       " 'black sun nighrebirth',\n",
       " 'blank',\n",
       " 'blank slate',\n",
       " 'blank slate editionprevious',\n",
       " 'blankets',\n",
       " 'blankets cold',\n",
       " 'blankets cold winter',\n",
       " 'bless',\n",
       " 'bless curse',\n",
       " 'bless curse hate',\n",
       " 'blessed',\n",
       " 'blessed thou',\n",
       " 'blessed thou art',\n",
       " 'blessings',\n",
       " 'blessings upon',\n",
       " 'blessings upon said',\n",
       " 'blitzed',\n",
       " 'blitzed polydrug',\n",
       " 'blitzed polydrug mix',\n",
       " 'blood',\n",
       " 'blood day',\n",
       " 'blood day aunt',\n",
       " 'blood movie',\n",
       " 'blood movie argument',\n",
       " 'blood numenor',\n",
       " 'blood numenor superior',\n",
       " 'blood numenoreans',\n",
       " 'blood numenoreans became',\n",
       " 'blood pretty',\n",
       " 'blood pretty sure',\n",
       " 'bloom',\n",
       " 'bloom talk',\n",
       " 'bloom talk gobbledygook',\n",
       " 'bloomer',\n",
       " 'bloomer meme',\n",
       " 'bloomer meme memes',\n",
       " 'blower',\n",
       " 'blower book',\n",
       " 'blower book sanitation',\n",
       " 'blows',\n",
       " 'blows bros',\n",
       " 'blue',\n",
       " 'blue haired',\n",
       " 'blue haired agents',\n",
       " 'bm',\n",
       " 'bm called',\n",
       " 'bm called voice',\n",
       " 'board',\n",
       " 'board edition',\n",
       " 'board edition rules',\n",
       " 'board wide',\n",
       " 'board wide reading',\n",
       " 'body',\n",
       " 'body occupies',\n",
       " 'body occupies expanse',\n",
       " 'book',\n",
       " 'book book',\n",
       " 'book book encouraged',\n",
       " 'book club',\n",
       " 'book club appears',\n",
       " 'book consider',\n",
       " 'book consider extraordinarily',\n",
       " 'book contains',\n",
       " 'book contains wisdom',\n",
       " 'book encouraged',\n",
       " 'book encouraged kind',\n",
       " 'book https',\n",
       " 'book https www',\n",
       " 'book internet',\n",
       " 'book internet impact',\n",
       " 'book poem',\n",
       " 'book poem dámaso',\n",
       " 'book review',\n",
       " 'book sanitation',\n",
       " 'book sanitation meat',\n",
       " 'book shopping',\n",
       " 'book shopping bros',\n",
       " 'book time',\n",
       " 'book unemployed',\n",
       " 'book unemployed classic',\n",
       " 'bookchin',\n",
       " 'bookchin instead',\n",
       " 'books',\n",
       " 'books alchemy',\n",
       " 'books alchemy looking',\n",
       " 'books buy',\n",
       " 'books buy 15k',\n",
       " 'books combining',\n",
       " 'books combining philosophy',\n",
       " 'books cope',\n",
       " 'books cope pussy',\n",
       " 'books curled',\n",
       " 'books curled wrapped',\n",
       " 'books david',\n",
       " 'books david lynch',\n",
       " 'books depressed',\n",
       " 'books depressed looking',\n",
       " 'books especially',\n",
       " 'books especially jon',\n",
       " 'books felt',\n",
       " 'books felt pic',\n",
       " 'books help',\n",
       " 'books help find',\n",
       " 'books help stop',\n",
       " 'books help tweets',\n",
       " 'books judging',\n",
       " 'books listening',\n",
       " 'books listening better',\n",
       " 'books read',\n",
       " 'books read books',\n",
       " 'books read kids',\n",
       " 'books reversal',\n",
       " 'books reversal gender',\n",
       " 'books starting',\n",
       " 'books starting cambridge',\n",
       " 'books taoism',\n",
       " 'books theme',\n",
       " 'books theme explored',\n",
       " 'books turn',\n",
       " 'books turn doomer',\n",
       " 'books worth',\n",
       " 'books worth reading',\n",
       " 'booksthis',\n",
       " 'booksthis fucking',\n",
       " 'booksthis fucking blows',\n",
       " 'boon',\n",
       " 'boon human',\n",
       " 'boon human thinking',\n",
       " 'boot',\n",
       " 'boot word',\n",
       " 'boot word processing',\n",
       " 'booting',\n",
       " 'booting computer',\n",
       " 'booting computer directly',\n",
       " 'border',\n",
       " 'border property',\n",
       " 'border property stop',\n",
       " 'borges',\n",
       " 'borges currently',\n",
       " 'borges currently 90',\n",
       " 'boring',\n",
       " 'boring please',\n",
       " 'boring please recommend',\n",
       " 'born',\n",
       " 'born late',\n",
       " 'born late find',\n",
       " 'borrowed',\n",
       " 'borrowed assume',\n",
       " 'borrowed assume substances',\n",
       " 'boy',\n",
       " 'boy homie',\n",
       " 'boy homie stirner',\n",
       " 'bradwardine',\n",
       " 'bradwardine theoretical',\n",
       " 'bradwardine theoretical geometrygerard',\n",
       " 'brahs',\n",
       " 'brahs loking',\n",
       " 'brahs loking piece',\n",
       " 'brain',\n",
       " 'brain chemistry',\n",
       " 'brain left',\n",
       " 'brain left fa',\n",
       " 'brained',\n",
       " 'brained unfocused',\n",
       " 'brained unfocused mode',\n",
       " 'brand',\n",
       " 'brand manuscript',\n",
       " 'brand manuscript strongly',\n",
       " 'breaks',\n",
       " 'breaks mostly',\n",
       " 'breaks mostly signify',\n",
       " 'bring',\n",
       " 'bring choices',\n",
       " 'bring choices reactions',\n",
       " 'bringing',\n",
       " 'bringing life',\n",
       " 'british',\n",
       " 'british english',\n",
       " 'british english non',\n",
       " 'broader',\n",
       " 'broader horizons',\n",
       " 'broader horizons beckoned',\n",
       " 'bronzeage',\n",
       " 'bronzeage ishhe',\n",
       " 'bronzeage ishhe decapitate',\n",
       " 'bros',\n",
       " 'bros miss',\n",
       " 'bros miss smell',\n",
       " 'browse',\n",
       " 'browse profile',\n",
       " 'browse profile loaded',\n",
       " 'brush',\n",
       " 'brush aristotle',\n",
       " 'brush aristotle anything',\n",
       " 'brusquely',\n",
       " 'brusquely asked',\n",
       " 'brusquely asked mother',\n",
       " 'buch',\n",
       " 'buch soll',\n",
       " 'buch soll ich',\n",
       " 'bugman',\n",
       " 'bugman could',\n",
       " 'bugman could tell',\n",
       " 'bugman literature',\n",
       " 'bugman literature main',\n",
       " 'bugman tier',\n",
       " 'bugman tier empiricism',\n",
       " 'build',\n",
       " 'build self',\n",
       " 'build self discipline',\n",
       " 'bullshit',\n",
       " 'bullshit anybody',\n",
       " 'bullshit anybody source',\n",
       " 'bunch',\n",
       " 'bunch dumb',\n",
       " 'bunch dumb people',\n",
       " 'burgerized',\n",
       " 'burgerized pussyllanimous',\n",
       " 'burgerized pussyllanimous bugman',\n",
       " 'burned',\n",
       " 'burned permanently',\n",
       " 'burned permanently memory',\n",
       " 'burning',\n",
       " 'burning black',\n",
       " 'burning black sun',\n",
       " 'businesses',\n",
       " 'businesses asteroid',\n",
       " 'businesses asteroid mines',\n",
       " 'buy',\n",
       " 'buy 15k',\n",
       " 'buy collected',\n",
       " 'buy collected stories',\n",
       " 'bücher',\n",
       " 'bücher empfehlen',\n",
       " 'bücher empfehlen die',\n",
       " 'calendars',\n",
       " 'calendars tonalpohualli',\n",
       " 'calendars tonalpohualli xiuhpohualli',\n",
       " 'call',\n",
       " 'call time',\n",
       " 'call time place',\n",
       " 'called',\n",
       " 'called chamber',\n",
       " 'called chamber house',\n",
       " 'called dominated',\n",
       " 'called dominated world',\n",
       " 'called litter',\n",
       " 'called litter ature',\n",
       " 'called tajweed',\n",
       " 'called tajweed similar',\n",
       " 'called voice',\n",
       " 'called voice ominous',\n",
       " 'cambridge',\n",
       " 'cambridge world',\n",
       " 'cambridge world prehistory',\n",
       " 'came',\n",
       " 'came accidentally',\n",
       " 'came accidentally fritz',\n",
       " 'came across',\n",
       " 'came across people',\n",
       " 'came messenger',\n",
       " 'came messenger allah',\n",
       " 'cancer',\n",
       " 'cancer love',\n",
       " 'cancer love god',\n",
       " 'cannot',\n",
       " 'cannot accepted',\n",
       " 'cannot accepted form',\n",
       " 'cannot exist',\n",
       " 'cannot exist except',\n",
       " 'cannot impo1',\n",
       " 'cannot impo1 rant',\n",
       " 'cannot live',\n",
       " 'cannot live augustine',\n",
       " 'cannot nowhere',\n",
       " 'cannot nowhere science',\n",
       " 'cannot reason',\n",
       " 'cannot reason rightly',\n",
       " 'canon',\n",
       " 'canon linearly',\n",
       " 'canon linearly debating',\n",
       " 'canon maybe',\n",
       " 'canon maybe 10',\n",
       " 'canon without',\n",
       " 'canon without whiny',\n",
       " 'cantillation',\n",
       " 'cantillation arberry',\n",
       " 'cantillation arberry fatiha',\n",
       " 'cap',\n",
       " 'cap society',\n",
       " 'cap society different',\n",
       " 'cap society stop',\n",
       " 'capitalism',\n",
       " 'capitalism account',\n",
       " 'capitalism account fact',\n",
       " 'capitalism critique',\n",
       " 'capitalism critique miss',\n",
       " 'capitalism socialism',\n",
       " 'capitalism socialism came',\n",
       " 'capitalist',\n",
       " 'capitalist frontier',\n",
       " 'capitalist frontier interplanetary',\n",
       " 'caravan',\n",
       " 'caravan another',\n",
       " 'caravan another worldby',\n",
       " 'care',\n",
       " 'care wittgenstein',\n",
       " 'care wittgenstein okay',\n",
       " 'carefully',\n",
       " 'carefully fritz',\n",
       " 'carefully fritz known',\n",
       " 'cares',\n",
       " 'cares goal',\n",
       " 'cares goal advancing',\n",
       " 'cartoons',\n",
       " 'cartoons literature',\n",
       " 'catch',\n",
       " 'catch heaven',\n",
       " 'catch heaven lassodominant',\n",
       " 'catcher',\n",
       " 'catcher rye',\n",
       " 'catcher rye considered',\n",
       " 'categories',\n",
       " 'catholic',\n",
       " 'catholic culture',\n",
       " 'catholic culture catholic',\n",
       " 'catholic culture general',\n",
       " 'catholic liturgical',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "books\n",
      "read\n",
      "books taoism\n",
      "taoism\n",
      "books read\n",
      "books read kids\n",
      "read kids\n",
      "kids\n",
      "philosophy\n",
      "help\n",
      " \n",
      "Topic 1:\n",
      "start\n",
      "lit\n",
      "philosophy\n",
      "book\n",
      "start philosophy\n",
      "greatest\n",
      "time\n",
      "reading\n",
      "last\n",
      "cinema\n",
      " \n",
      "Topic 2:\n",
      "start\n",
      "philosophy\n",
      "start philosophy\n",
      "philosopher\n",
      "australia\n",
      "australia philosophy\n",
      "consider greatest\n",
      "consider greatest philosopher\n",
      "country\n",
      "country start\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print(\"Topic %d:\" % i )\n",
    "    for term in sortedTerms:\n",
    "        print(term[0])\n",
    "    print (\" \")\n",
    "# These words are the most important features for a topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>INFERENCE</center></h1>\n",
    "The hot topics in Literature have been unearthed from the posts on the website. Tha main topics are : \n",
    "\n",
    "1. Topic 1 discusses Kids books and Taoism (Chinese Philosopy)\n",
    "   <font color='blue'>Some instances from documents assigned to this topic - What are the best books on Taoism</font>\n",
    "   \n",
    "   \n",
    "2. Topic 2 discusses general philosophy, book and cinema\n",
    "   <font color='blue'> Some instances from documents assigned to this topic - Has cinema surpassed the novel?, Humanism has changed drastically since the 17th century, only someone who can&#039;t close read novels or engage with philosophy would come to this conclusion. Many modernist writers were immensely influenced by early cinema</font>\n",
    "   \n",
    "   \n",
    "3. Topic 3 is discusses australia philosophy and greatest philosopher\n",
    "<font color='blue'> Some instances from documents assigned to this topic -I thought he was British. But yeah, John Finnis is likely the greatest Australian philosopher.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>NEXT STEPS</center></h1>\n",
    "1. Tag every document to a topic (Eigen vector) based on any nearest neighbor methods\n",
    "\n",
    "2. We can see that the topics discussed are mostly chinese and australian philosophy. I would like to look at the demographic information of the users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
